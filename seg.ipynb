{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5e8ba21f584e888a6fb40e31a81775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/famer.me/Repository/detr-playground/.venv/lib/python3.9/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/famer.me/Repository/detr-playground/.venv/lib/python3.9/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38489b2415c342009ebfe5dde72b86aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/70.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf26266a4f6400aa558e1780f4d4841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.classifier.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b5\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_url = \"https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png\"\n",
    "example_image = Image.open(requests.get(example_image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cityscapes_labels = [\n",
    "    \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \n",
    "    \"pole\", \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\",\n",
    "    \"sky\", \"person\", \"rider\", \"car\", \"truck\", \n",
    "    \"bus\", \"train\", \"motorcycle\", \"bicycle\"\n",
    "]\n",
    "colors = np.random.randint(0, 255, size=(len(cityscapes_labels), 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 607 is out of bounds for axis 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create an RGB image for each class (you might need to customize this part based on the number of classes and desired colors)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# random 14 colors : np.array([[0, 0, 0], ...]) for each class\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m seg_image \u001b[38;5;241m=\u001b[39m \u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_np\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Overlay (this can be adjusted to change the transparency or the way of overlaying)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m \u001b[38;5;241m*\u001b[39m image_np \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m seg_image\n",
      "\u001b[0;31mIndexError\u001b[0m: index 607 is out of bounds for axis 0 with size 19"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# image = example_image\n",
    "image = Image.open(\"./datasets/Das3300161.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "\n",
    "# Overlay the mask on the original image\n",
    "# Convert original image to numpy array and normalize it\n",
    "image_np = np.array(image) / 255.0\n",
    "\n",
    "# Apply softmax and argmax to get the most likely class for each pixel\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "mask = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "# Resize mask to the size of the original image\n",
    "mask = F.resize(mask.unsqueeze(1), size=image_np.shape[:2], interpolation=F.InterpolationMode.NEAREST).squeeze()\n",
    "\n",
    "# Convert mask to numpy array for visualization\n",
    "mask_np = mask.detach().cpu().numpy()\n",
    "\n",
    "# Create an RGB image for each class (you might need to customize this part based on the number of classes and desired colors)\n",
    "# random 14 colors : np.array([[0, 0, 0], ...]) for each class\n",
    "\n",
    "seg_image = colors[mask_np]\n",
    "\n",
    "# Overlay (this can be adjusted to change the transparency or the way of overlaying)\n",
    "combined = 0.7 * image_np + 0.001 * seg_image\n",
    "\n",
    "# Create a subplot: 1 row, 3 columns\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axs[0].imshow(image_np)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Segmentation mask\n",
    "axs[1].imshow(seg_image)\n",
    "axs[1].set_title('Segmentation Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Combined image\n",
    "axs[2].imshow(combined)\n",
    "axs[2].set_title('Overlay Image')\n",
    "axs[2].axis('off')\n",
    "\n",
    "# filter label in output include in cityscapes_labels\n",
    "unique_labels = np.unique(mask_np)\n",
    "\n",
    "# Add the legend to the last subplot, convert colors to 0-1\n",
    "patches = [mpatches.Patch(color=colors[i]/255.0, label=cityscapes_labels[i]) for i in range(len(cityscapes_labels)) if i in unique_labels]\n",
    "axs[2].legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "# Display the subplot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'building', 'car', 'crack', 'dog', 'litter', 'pavement', 'truck', 'tyer'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = \"/Users/famer.me/Repository/crack-in-asphalt/2023-12-04/Origin/1\"\n",
    "\n",
    "labels = [\n",
    "\tfile for file in os.listdir(datasets_dir) if file.endswith(\".json\")\n",
    "]\n",
    "\n",
    "images = [\n",
    "\tfile[0:-5] + \".jpg\" for file in labels\n",
    "]\n",
    "\n",
    "list(zip(labels, images))\n",
    "\n",
    "labels_json = [\n",
    "\tjson.load(open(os.path.join(datasets_dir, label), \"r\")) for label in labels\n",
    "]\n",
    "\n",
    "set([\n",
    "\tshape[\"label\"]\n",
    "\tfor label_json in labels_json\n",
    "\tfor shape in label_json[\"shapes\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'flags', 'shapes', 'imagePath', 'imageData', 'imageHeight', 'imageWidth'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_json[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
