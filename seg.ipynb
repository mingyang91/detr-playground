{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T09:56:54.552271104Z",
     "start_time": "2024-02-02T09:56:51.064256949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yang.ming/miniconda3/envs/detr-playground/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.linear_fuse.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.bias', 'decode_head.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/mit-b5\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_url = \"https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png\"\n",
    "example_image = Image.open(requests.get(example_image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cityscapes_labels = [\n",
    "    \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \n",
    "    \"pole\", \"traffic light\", \"traffic sign\", \"vegetation\", \"terrain\",\n",
    "    \"sky\", \"person\", \"rider\", \"car\", \"truck\", \n",
    "    \"bus\", \"train\", \"motorcycle\", \"bicycle\"\n",
    "]\n",
    "colors = np.random.randint(0, 255, size=(len(cityscapes_labels), 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 607 is out of bounds for axis 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create an RGB image for each class (you might need to customize this part based on the number of classes and desired colors)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# random 14 colors : np.array([[0, 0, 0], ...]) for each class\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m seg_image \u001b[38;5;241m=\u001b[39m \u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_np\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Overlay (this can be adjusted to change the transparency or the way of overlaying)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m \u001b[38;5;241m*\u001b[39m image_np \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m seg_image\n",
      "\u001b[0;31mIndexError\u001b[0m: index 607 is out of bounds for axis 0 with size 19"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# image = example_image\n",
    "image = Image.open(\"./datasets/Das3300161.jpg\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "\n",
    "# Overlay the mask on the original image\n",
    "# Convert original image to numpy array and normalize it\n",
    "image_np = np.array(image) / 255.0\n",
    "\n",
    "# Apply softmax and argmax to get the most likely class for each pixel\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "mask = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "# Resize mask to the size of the original image\n",
    "mask = F.resize(mask.unsqueeze(1), size=image_np.shape[:2], interpolation=F.InterpolationMode.NEAREST).squeeze()\n",
    "\n",
    "# Convert mask to numpy array for visualization\n",
    "mask_np = mask.detach().cpu().numpy()\n",
    "\n",
    "# Create an RGB image for each class (you might need to customize this part based on the number of classes and desired colors)\n",
    "# random 14 colors : np.array([[0, 0, 0], ...]) for each class\n",
    "\n",
    "seg_image = colors[mask_np]\n",
    "\n",
    "# Overlay (this can be adjusted to change the transparency or the way of overlaying)\n",
    "combined = 0.7 * image_np + 0.001 * seg_image\n",
    "\n",
    "# Create a subplot: 1 row, 3 columns\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axs[0].imshow(image_np)\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Segmentation mask\n",
    "axs[1].imshow(seg_image)\n",
    "axs[1].set_title('Segmentation Mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Combined image\n",
    "axs[2].imshow(combined)\n",
    "axs[2].set_title('Overlay Image')\n",
    "axs[2].axis('off')\n",
    "\n",
    "# filter label in output include in cityscapes_labels\n",
    "unique_labels = np.unique(mask_np)\n",
    "\n",
    "# Add the legend to the last subplot, convert colors to 0-1\n",
    "patches = [mpatches.Patch(color=colors[i]/255.0, label=cityscapes_labels[i]) for i in range(len(cityscapes_labels)) if i in unique_labels]\n",
    "axs[2].legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "# Display the subplot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'building', 'car', 'crack', 'dog', 'litter', 'pavement', 'truck', 'tyer'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = \"/Users/famer.me/Repository/crack-in-asphalt/2023-12-04/Origin/1\"\n",
    "\n",
    "labels = [\n",
    "\tfile for file in os.listdir(datasets_dir) if file.endswith(\".json\")\n",
    "]\n",
    "\n",
    "images = [\n",
    "\tfile[0:-5] + \".jpg\" for file in labels\n",
    "]\n",
    "\n",
    "list(zip(labels, images))\n",
    "\n",
    "labels_json = [\n",
    "\tjson.load(open(os.path.join(datasets_dir, label), \"r\")) for label in labels\n",
    "]\n",
    "\n",
    "set([\n",
    "\tshape[\"label\"]\n",
    "\tfor label_json in labels_json\n",
    "\tfor shape in label_json[\"shapes\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'flags', 'shapes', 'imagePath', 'imageData', 'imageHeight', 'imageWidth'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T09:57:02.586540914Z",
     "start_time": "2024-02-02T09:57:02.397540900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "processor = SegformerImageProcessor.from_pretrained('mingyang91/segformer-for-surveillance')\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"mingyang91/segformer-for-surveillance\").to('mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from segformer_trainer import downsample_image_pil, polygons_to_mask, split_image_into_tiles, split_mask_into_tiles\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "basedir = 'datasets/segformer/val'\n",
    "test = 'Das1100051.json'\n",
    "scale_factor = 4.0\n",
    "\n",
    "ann_path = os.path.join(basedir, test)\n",
    "img_path = os.path.join(basedir, test.replace('.json', '.jpg'))\n",
    "\n",
    "# Load image\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "image = downsample_image_pil(image, scale_factor=scale_factor)\n",
    "image = np.array(image)\n",
    "\n",
    "# Load annotation and create mask\n",
    "with open(ann_path) as f:\n",
    "    anns = json.load(f)\n",
    "for shape in anns['shapes']:\n",
    "    if shape['shape_type'] == 'polygon':\n",
    "        shape['points'] = [[int(p[0] / scale_factor), int(p[1] / scale_factor)] for p in shape['points']]\n",
    "mask = polygons_to_mask(image.shape[:2], anns['shapes'])\n",
    "\n",
    "tiles = split_image_into_tiles(image)\n",
    "\n",
    "# Optionally split the mask here in the same way if training\n",
    "masks = split_mask_into_tiles(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "all_image_tiles = [processor(images=item, return_tensors=\"pt\")['pixel_values'].squeeze() for item in tiles]\n",
    "all_mask_tiles = [torch.as_tensor(item, dtype=torch.long) for item in masks]\n",
    "\n",
    "# Stack the selected tensors to create batches\n",
    "images_stacked = torch.stack(all_image_tiles).to('mps')\n",
    "masks_stacked = torch.stack(all_mask_tiles).to('mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from segformer_trainer import id2label\n",
    "for i in range(len(images_stacked)):\n",
    "\toutputs = model(pixel_values=images_stacked[i].unsqueeze(0), labels=masks_stacked[i].unsqueeze(0))\n",
    "\tlogits = outputs.logits\n",
    "\tprobabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "\tmask = torch.argmax(probabilities, dim=1)\n",
    "\tmask = mask.cpu().numpy()\n",
    "\tplt.imshow(mask.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
